{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model with simpler data and Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/robert/software/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import re\n",
    "import logging \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import scale\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pickle\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "lg = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating some toy data to play around"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sim(n, p, null_prop):\n",
    "    lg.info('n: %s, p: %s', n, p)\n",
    "    pred = list()\n",
    "    for i in range(p):\n",
    "        pred.append(np.random.normal(0, 1, n))\n",
    "    pred = np.column_stack(pred)\n",
    "    beta = np.random.normal(0, 0.1**2, p)\n",
    "    null_index = np.random.choice(range(p),int(p*null_prop),\n",
    "                     replace=False)\n",
    "    beta[null_index] = 0\n",
    "    y = scale(pred.dot(beta.T))\n",
    "    y = scale(y) + np.random.normal(0, 1, n)\n",
    "    return y, pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n = 10000\n",
    "# p = 10000\n",
    "# null_prop = 0.99\n",
    "# y, pred = sim(n, p, null_prop)\n",
    "# alpha_values = np.arange(0.001, 0.01, 0.001)\n",
    "# lg.info('Number of alphas: %s', alpha_values)\n",
    "# x_train, x_test, y_train, y_test = train_test_split(pred, y,\n",
    "#                                                     test_size=0.1,\n",
    "#                                                     random_state=42)\n",
    "# n_train = x_train.shape[0]\n",
    "# y_train = y_train.reshape(n_train, 1)\n",
    "# del pred\n",
    "# num_blocks = 4\n",
    "# blocks = [[k for k in range(l)] for l in np.repeat(int(p/num_blocks), num_blocks)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:There are 25 file present\n",
      "INFO:__main__:There are 85 LD blocks to process\n",
      "INFO:__main__:There are 2504 subjects\n",
      "INFO:__main__:There are 405378 SNPs\n"
     ]
    }
   ],
   "source": [
    "lg = logging.getLogger(__name__)\n",
    "logging.basicConfig(level=logging.DEBUG)\n",
    "\n",
    "batch_folder = '../data/sample_major_1kg/'\n",
    "batch_files = os.listdir(batch_folder)\n",
    "batch_files = [os.path.join(batch_folder, k) for k in batch_files]\n",
    "lg.info('There are %s file present', len(batch_files))\n",
    "\n",
    "blocks = pickle.load(open('../data/sim_1000G_chr10.ld_blocks.pickel', 'rb'))\n",
    "blocks = blocks[10] # remove the chromosom\n",
    "lg.info('There are %s LD blocks to process', len(blocks))\n",
    "\n",
    "fam = pd.read_table('../data/sim_1000G_chr10.txt')\n",
    "pheno = fam['V3'].values\n",
    "sub_pheno = pheno[0:100]\n",
    "sub_pheno = sub_pheno.reshape(100, 1)\n",
    "lg.info('There are %s subjects', len(pheno))\n",
    "\n",
    "bim = pd.read_table('../data/sim_1000G_chr10.bim', header=None)\n",
    "snps = bim[1].values\n",
    "p = len(snps)\n",
    "lg.info('There are %s SNPs', len(snps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate a bool list with the block seperation\n",
    "def make_block_id(blocks, p):\n",
    "    output = list()\n",
    "    u = 0\n",
    "    for i, b in enumerate(blocks):\n",
    "        nn = len(b)\n",
    "        mask = np.zeros(p, dtype=bool)\n",
    "        mask[u:(u + nn)] = True\n",
    "        u += nn\n",
    "        output.append(mask)\n",
    "        if i % 10 == 0:\n",
    "            lg.debug('Processing LD block %s', i)\n",
    "    return output\n",
    "\n",
    "# def make_block_id(snps, blocks):\n",
    "#     output = list()\n",
    "#     u = 0\n",
    "#     for i, b in enumerate(blocks):\n",
    "#         nn = len(b)\n",
    "#         mask = np.zeros(len(snps), dtype=bool)\n",
    "#         mask[u:(u+nn)] = True\n",
    "#         u+=nn\n",
    "#         output.append(mask)\n",
    "#         if i % 10 == 0:\n",
    "#             lg.debug('Processing LD block %s', i)\n",
    "#     return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:__main__:Processing LD block 0\n",
      "DEBUG:__main__:Processing LD block 10\n",
      "DEBUG:__main__:Processing LD block 20\n",
      "DEBUG:__main__:Processing LD block 30\n",
      "DEBUG:__main__:Processing LD block 40\n",
      "DEBUG:__main__:Processing LD block 50\n",
      "DEBUG:__main__:Processing LD block 60\n",
      "DEBUG:__main__:Processing LD block 70\n",
      "DEBUG:__main__:Processing LD block 80\n"
     ]
    }
   ],
   "source": [
    "bool_blocks = make_block_id(blocks, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def n_iterator(X, y, chunk_size=100):\n",
    "    n, p = X.shape\n",
    "    grouped = list(zip(*[range(n)] * chunk_size))\n",
    "    for i in grouped:\n",
    "        yield X[i,:].reshape(chunk_size, p) , y[i,:].reshape(chunk_size, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:25 files in total, allocating 22 for training and 3 for validation\n"
     ]
    }
   ],
   "source": [
    "dirfold = '../data/sample_major_1kg/'\n",
    "files = os.listdir(dirfold)\n",
    "files = [os.path.join(dirfold, k) for k in files]\n",
    "training_files = np.random.choice(files, replace=False, size=int(len(files)* 0.9))\n",
    "test_files = [l for l in files if l not in training_files]\n",
    "lg.info('%s files in total, allocating %s for training and %s for validation', len(files), len(training_files), len(test_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../data/sample_major_1kg/sample_major_21.npy',\n",
       " '../data/sample_major_1kg/sample_major_0.npy',\n",
       " '../data/sample_major_1kg/sample_major_16.npy']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['../data/sample_major_1kg/sample_major_8.npy',\n",
       "       '../data/sample_major_1kg/sample_major_18.npy',\n",
       "       '../data/sample_major_1kg/sample_major_3.npy',\n",
       "       '../data/sample_major_1kg/sample_major_15.npy',\n",
       "       '../data/sample_major_1kg/sample_major_13.npy',\n",
       "       '../data/sample_major_1kg/sample_major_7.npy',\n",
       "       '../data/sample_major_1kg/sample_major_17.npy',\n",
       "       '../data/sample_major_1kg/sample_major_4.npy',\n",
       "       '../data/sample_major_1kg/sample_major_23.npy',\n",
       "       '../data/sample_major_1kg/sample_major_9.npy',\n",
       "       '../data/sample_major_1kg/sample_major_22.npy',\n",
       "       '../data/sample_major_1kg/sample_major_12.npy',\n",
       "       '../data/sample_major_1kg/sample_major_14.npy',\n",
       "       '../data/sample_major_1kg/sample_major_10.npy',\n",
       "       '../data/sample_major_1kg/sample_major_1.npy',\n",
       "       '../data/sample_major_1kg/sample_major_5.npy',\n",
       "       '../data/sample_major_1kg/sample_major_6.npy',\n",
       "       '../data/sample_major_1kg/sample_major_11.npy',\n",
       "       '../data/sample_major_1kg/sample_major_20.npy',\n",
       "       '../data/sample_major_1kg/sample_major_24.npy',\n",
       "       '../data/sample_major_1kg/sample_major_19.npy',\n",
       "       '../data/sample_major_1kg/sample_major_2.npy'], dtype='<U44')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def geno_iterator(paths, y):\n",
    "    np.random.shuffle(paths)\n",
    "    for p in paths:\n",
    "        data, index_vec = np.load(p)\n",
    "        n, p = data.shape\n",
    "        yield data, y[index_vec].reshape(n, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define TensorFlow model \n",
    "\n",
    "I used placeholder since they are more flexible in regards to batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xp = tf.placeholder(tf.float32, [None, p], name='X')\n",
    "yp = tf.placeholder(tf.float32, [None, 1], name='y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:__main__:generated variables for LD block #0\n",
      "DEBUG:__main__:generated variables for LD block #10\n",
      "DEBUG:__main__:generated variables for LD block #20\n",
      "DEBUG:__main__:generated variables for LD block #30\n",
      "DEBUG:__main__:generated variables for LD block #40\n",
      "DEBUG:__main__:generated variables for LD block #50\n",
      "DEBUG:__main__:generated variables for LD block #60\n",
      "DEBUG:__main__:generated variables for LD block #70\n",
      "DEBUG:__main__:generated variables for LD block #80\n"
     ]
    }
   ],
   "source": [
    "batch_size = Xp.get_shape()[0]\n",
    "rand_norm_init = tf.initializers.random_normal(0, 0.0001)\n",
    "linear_combiner = tf.constant(1.0, shape=[len(bool_blocks), 1])\n",
    "# define initial linear layer for each block\n",
    "with tf.variable_scope('Genotypes'):\n",
    "    collector = list()\n",
    "    for i, b in enumerate(bool_blocks):\n",
    "        out_list = list()\n",
    "        l1 = tf.contrib.layers.l1_regularizer(scale=0.005, scope=None)\n",
    "        with tf.variable_scope('LD_block'+ str(i)):\n",
    "            small_block = tf.boolean_mask(Xp, b, axis=1)\n",
    "            small_block.set_shape((batch_size, np.sum(b)))\n",
    "            y_ = tf.layers.dense(small_block, 1, kernel_regularizer=l1, kernel_initializer=rand_norm_init)\n",
    "            collector.append(y_)\n",
    "            if i % 10 == 0:\n",
    "                lg.debug('generated variables for LD block #%s', i)\n",
    "        \n",
    "# define neural layers\n",
    "collection = tf.concat(collector, name='prediction_matrix', axis=1)\n",
    "# n1 = tf.layers.dense(collection, 85, name='n1', kernel_initializer=rand_norm_init)\n",
    "# n2 = tf.layers.dense(n1, 40, name='n2', kernel_initializer=rand_norm_init)\n",
    "# n3 = tf.layers.dense(n2, 20, name='n3', kernel_initializer=rand_norm_init)\n",
    "# y_hat = tf.layers.dense(n3, 1, name='output_layer', kernel_initializer=rand_norm_init)\n",
    "y_hat = tf.matmul(collection, linear_combiner, name='combinging_linear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = tf.losses.mean_squared_error(yp, y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer = tf.train.GradientDescentOptimizer(0.0001).minimize(mse)\n",
    "optimizer = tf.train.AdagradOptimizer(0.001).minimize(mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "now = datetime.now()\n",
    "now = now.strftime(\"%Y%m%d-%H%M%S\") \n",
    "l1_loss = tf.losses.get_regularization_loss()\n",
    "mse += l1_loss\n",
    "accuracy = tf.contrib.metrics.streaming_pearson_correlation(y_hat, yp, name='correlation')\n",
    "\n",
    "sum_accu = tf.summary.scalar('Accuracy', accuracy[1])\n",
    "sum_loss = tf.summary.scalar('Loss', mse)\n",
    "sum_l1 = tf.summary.scalar('L1_loss', l1_loss)\n",
    "\n",
    "merged = tf.summary.merge_all()\n",
    "\n",
    "train_writer = tf.summary.FileWriter('tensorboard/neural_network/train'+now, tf.get_default_graph())\n",
    "test_writer = tf.summary.FileWriter('tensorboard/neural_network/test'+now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "re_assemble_testing = True\n",
    "if re_assemble_testing:\n",
    "    x_test = list()\n",
    "    y_test = list()\n",
    "    genoiter = geno_iterator(test_files, pheno)\n",
    "    for x, y in genoiter:\n",
    "        x_test.append(x)\n",
    "        y_test.append(y)\n",
    "    x_test = np.concatenate(x_test, axis=0)\n",
    "    y_test = np.concatenate(y_test, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-223956d851b9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdat\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m             \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msummary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmerged\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mXp\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myp\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0mtrain_writer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_summary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m10\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/software/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    898\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 900\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    901\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/software/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1102\u001b[0m             \u001b[0mfeed_handles\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msubfeed_t\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msubfeed_val\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1103\u001b[0m           \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1104\u001b[0;31m             \u001b[0mnp_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubfeed_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msubfeed_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1106\u001b[0m           if (not is_tensor_handle_feed and\n",
      "\u001b[0;32m~/software/anaconda3/lib/python3.6/site-packages/numpy/core/numeric.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m    490\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m     \"\"\"\n\u001b[0;32m--> 492\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    493\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "init = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for i in range(400):\n",
    "        dat = geno_iterator(training_files, pheno)\n",
    "        pred = list()\n",
    "        for x, y in dat:\n",
    "            _, c, summary = sess.run([optimizer, mse, merged], feed_dict={Xp: x, yp: y})\n",
    "        train_writer.add_summary(summary, i)\n",
    "        if i % 10 == 0:\n",
    "            summary = sess.run(merged, feed_dict={Xp: x_test, yp: y_test})\n",
    "            test_writer.add_summary(summary, i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "12px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
